{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZEUaYSjC6CZ"
   },
   "source": [
    "# Constructing a high-level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5uAmHirHtUwd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install --quiet scvi-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tcMSn-GSxAeC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scvi\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7a98f8MiykA"
   },
   "source": [
    "At this point we have covered\n",
    "\n",
    "1. Data registration via `scvi.data.setup_anndata` and dataloaders via `AnnDataLoader`\n",
    "2. Building a probabilistic model by subclassing `BaseModuleClass`\n",
    "\n",
    "In this tutorial, we will cover the highest-level classes in `scvi-tools`: the model classes. The main purpose of these classes (e.g., `scvi.model.SCVI`) is to wrap the actions of module instantiation, training, and subsequent posterior queries of our module into a convenient interface. These model classes are the fundamental objects driving scientific analysis of data with `scvi-tools`. Out of convention, we will refer to these objects as \"models\" and the lower-level objects presented in the previous tutorial as \"modules\".\n",
    "\n",
    "## A simple model class\n",
    "Here we will walkthrough an example of building the `scvi.model.SCVI` class. We will progressively add functionality to the class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUwavqvn0BBk"
   },
   "source": [
    "\n",
    "### Sketch of `BaseModelClass`\n",
    "\n",
    "Let us start by providing a high level overview of `BaseModelClass` that we will inherit. Note that this is pseudocode to provide intuition. We see that `BaseModelClass` contains some unverisally applicable methods, and some private methods (conventionally starting with `_` in Python) that will become useful after training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_rabB4JoGG3"
   },
   "source": [
    "```python\n",
    "class MyModel(UnsupervisedTrainingMixin, BaseModelClass)\n",
    "\n",
    "    def __init__(self, adata):\n",
    "        # sets some basic attributes like is_trained_\n",
    "        # record the setup_dict registered in the adata\n",
    "        self.adata = adata\n",
    "        self.scvi_setup_dict_ = adata.uns[\"_scvi\"]\n",
    "        self.summary_stats = self.scvi_setup_dict_[\"summary_stats\"]\n",
    "\n",
    "    def _validate_anndata(self, adata):\n",
    "        # check that anndata is equivalent by comparing\n",
    "        # to the initial setup_dict\n",
    "\n",
    "    def _make_dataloader(adata):\n",
    "        # return a dataloader to iterate over adata\n",
    "        \n",
    "    def train(...):\n",
    "        # Universal train method provided by UnsupservisedTrainingMixin\n",
    "        # BaseModelClass does not come with train\n",
    "        # In general train methods are straightforward to compose manually\n",
    "\n",
    "    def save(...):\n",
    "        # universal save method\n",
    "        # saves modules, anndata setup dict, and attributes ending with _\n",
    "\n",
    "    def load(...):\n",
    "        # universal load method\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vvYzZa_ukC4"
   },
   "source": [
    "### Baseline version of `SCVI` class\n",
    "\n",
    "Let's now create the simplest possible version of the `SCVI` class. We inherit the `BaseModelClass`, and write our `__init__` method.\n",
    "\n",
    "We take care to do the following:\n",
    "\n",
    "1. Set the `module` attribute to be equal to our `VAE` module, which here is the torch-level version of scVI.\n",
    "2. Add a `_model_summary_string` attr, which will be used as a representation for the model.\n",
    "3. Run `self.init_params_ = self._get_init_params(locals())`, which stores the arguments used to initialize the model, facilitating saving/loading of the model later.\n",
    "\n",
    "To initialize the `VAE`, we can use the information in `self.summary_stats`, which is information that was stored in the anndata object at `setup_anndata()` time. In this example, we have only exposed `n_latent` to users through `SCVI`. In practice, we try to expose only the most relevant parameters, as all other parameters can be accessed by passing `model_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q0zQzJD4jNoV"
   },
   "outputs": [],
   "source": [
    "from anndata import AnnData\n",
    "from scvi.module import VAE\n",
    "from scvi.model.base import BaseModelClass, UnsupervisedTrainingMixin\n",
    "\n",
    "class SCVI(UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"\n",
    "    single-cell Variational Inference [Lopez18]_.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_latent: int = 10,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super(SCVI, self).__init__(adata)\n",
    "\n",
    "        self.module = VAE(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            n_batch=self.summary_stats[\"n_batch\"],\n",
    "            n_latent=n_latent,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            \"SCVI Model with the following params: \\nn_latent: {}\"\n",
    "        ).format(\n",
    "            n_latent,\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qw-uCsuwhgO"
   },
   "source": [
    "Now we explore what we can and cannot do with this model. Let's get some data and initialize a `SCVI` instance. Of note, for testing purposes we like to use `scvi.data.synthetic_iid()` which returns a simple, small anndata object that was already run through `setup_anndata()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tw3DQF9IwftC",
    "outputId": "1e992a71-7a48-4881-8af8-9fec60138d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Using batches from adata.obs\u001b[1m[\u001b[0m\u001b[32m\"batch\"\u001b[0m\u001b[1m]\u001b[0m                                               \n",
      "\u001b[34mINFO    \u001b[0m Using labels from adata.obs\u001b[1m[\u001b[0m\u001b[32m\"labels\"\u001b[0m\u001b[1m]\u001b[0m                                               \n",
      "\u001b[34mINFO    \u001b[0m Using data from adata.X                                                             \n",
      "\u001b[34mINFO    \u001b[0m Computing library size prior per batch                                              \n",
      "\u001b[34mINFO    \u001b[0m Using protein expression from adata.obsm\u001b[1m[\u001b[0m\u001b[32m'protein_expression'\u001b[0m\u001b[1m]\u001b[0m                      \n",
      "\u001b[34mINFO    \u001b[0m Using protein names from adata.uns\u001b[1m[\u001b[0m\u001b[32m'protein_names'\u001b[0m\u001b[1m]\u001b[0m                                 \n",
      "\u001b[34mINFO    \u001b[0m Successfully registered anndata object containing \u001b[1;34m400\u001b[0m cells, \u001b[1;34m100\u001b[0m vars, \u001b[1;34m2\u001b[0m batches, \u001b[1;34m3\u001b[0m \n",
      "         labels, and \u001b[1;34m100\u001b[0m proteins. Also registered \u001b[1;34m0\u001b[0m extra categorical covariates and \u001b[1;34m0\u001b[0m extra\n",
      "         continuous covariates.                                                              \n",
      "\u001b[34mINFO    \u001b[0m Please do not further modify adata until model is trained.                          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 400 × 100\n",
       "    obs: 'batch', 'labels', '_scvi_batch', '_scvi_labels', '_scvi_local_l_mean', '_scvi_local_l_var'\n",
       "    uns: 'protein_names', '_scvi'\n",
       "    obsm: 'protein_expression'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = scvi.data.synthetic_iid()\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAg0lsNpuCYG",
    "outputId": "4d3b639f-39fe-42cf-cb9c-d2f16c9d0018"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCVI Model with the following params: \n",
       "n_latent: <span style=\"color: #000080; font-weight: bold\">10</span>\n",
       "Training status: Not Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fed6af02dc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "To print summary of associated AnnData, use: <span style=\"color: #af00d7\">scvi.data.view_anndata_setup(model.adata)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fed6af02df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SCVI(adata)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOau9VkQxyIp",
    "outputId": "34538c4d-bfdb-4ba5-d1be-68a045dd5c30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 20/20 [00:00<00:00, 42.08it/s, loss=305, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "model.train(max_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG1AU4cI0Fe1"
   },
   "source": [
    "### The `train` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDFKIKMdx-TO"
   },
   "source": [
    "We were able to train this model, as this method is inherited in the class. Let us now take a look at psedocode of the `train` method of `UnsupervisedTrainingMixin`. The function of each of these objects is described in the API reference.\n",
    "\n",
    "```python\n",
    "    def train(\n",
    "        self,\n",
    "        max_epochs: Optional[int] = 100,\n",
    "        use_gpu: Optional[bool] = None,\n",
    "        train_size: float = 0.9,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \"\"\"\n",
    "        # object to make train/test/val dataloaders\n",
    "        data_splitter = DataSplitter(\n",
    "            self.adata,\n",
    "            train_size=train_size,\n",
    "            validation_size=validation_size,\n",
    "            batch_size=batch_size,\n",
    "            use_gpu=use_gpu,\n",
    "        )\n",
    "        # defines optimizers, training step, val step, logged metrics\n",
    "        training_plan = TrainingPlan(\n",
    "            self.module, len(data_splitter.train_idx),\n",
    "        )\n",
    "        # creates Trainer, pre and post training procedures (Trainer.fit())\n",
    "        runner = TrainRunner(\n",
    "            self,\n",
    "            training_plan=training_plan,\n",
    "            data_splitter=data_splitter,\n",
    "            max_epochs=max_epochs,\n",
    "            use_gpu=use_gpu,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return runner()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fT3-7MQ0S6G"
   },
   "source": [
    "We notice two new things:\n",
    "\n",
    "1. A training plan (`training_plan`)\n",
    "2. A train runner (`runner`)\n",
    "\n",
    "The `TrainRunner` is a lightweight wrapper of the PyTorch lightning's [`Trainer`](https://pytorch-lightning.readthedocs.io/en/stable/trainer.html#trainer-class-api), which is a completely black-box method once a `TrainingPlan` is defined. So what does the `TrainingPlan` do?\n",
    "\n",
    "1. Configures optimizers (e.g., Adam), learning rate schedulers.\n",
    "2. Defines the training step, which runs a minibatch of data through the model and records the loss.\n",
    "3. Defines the validation step, same as training step, but for validation data.\n",
    "4. Records relevant metrics, such as the ELBO.\n",
    "\n",
    "In `scvi-tools` we have `scvi.lightning.TrainingPlan`, which should cover many use cases, from VAEs and VI, to MLE and MAP estimation. Developers may find that they need a custom `TrainingPlan` for e.g,. multiple optimizers and complex training scheme. These can be written and used by the model class.\n",
    "\n",
    "Developers may also overwrite this train method to add custom functionality like Early Stopping (see TOTALVI's train method). In most cases the higher-level train method can call `super().train()`, which would be the `BaseModelClass` train method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEqgLc8c2-Gf"
   },
   "source": [
    "### Save and load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3aR7N5O3BKR"
   },
   "source": [
    "We can also save and load this model object, as it follows the expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kjJoYZ42_7h",
    "outputId": "51244e0d-30af-4fa5-9ecc-9e837deced37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Using data from adata.X                                                             \n",
      "\u001b[34mINFO    \u001b[0m Computing library size prior per batch                                              \n",
      "\u001b[34mINFO    \u001b[0m Registered keys:\u001b[1m[\u001b[0m\u001b[32m'X'\u001b[0m, \u001b[32m'batch_indices'\u001b[0m, \u001b[32m'local_l_mean'\u001b[0m, \u001b[32m'local_l_var'\u001b[0m, \u001b[32m'labels'\u001b[0m,     \n",
      "         \u001b[32m'protein_expression'\u001b[0m\u001b[1m]\u001b[0m                                                               \n",
      "\u001b[34mINFO    \u001b[0m Successfully registered anndata object containing \u001b[1;34m400\u001b[0m cells, \u001b[1;34m100\u001b[0m vars, \u001b[1;34m2\u001b[0m batches, \u001b[1;34m3\u001b[0m \n",
      "         labels, and \u001b[1;34m100\u001b[0m proteins. Also registered \u001b[1;34m0\u001b[0m extra categorical covariates and \u001b[1;34m0\u001b[0m extra\n",
      "         continuous covariates.                                                              \n"
     ]
    }
   ],
   "source": [
    "model.save(\"saved_model/\", save_anndata=True)\n",
    "model = SCVI.load(\"saved_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAGP3_R23TnF"
   },
   "source": [
    "## Writing methods to query the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsTrpi2GCuRW"
   },
   "source": [
    "So we have a model that wraps a module that has been trained. How can we get information out of the module and present in cleanly to our users? Let's implement a simple example: getting the latent representation out of the VAE.\n",
    "\n",
    "This method has the following structure:\n",
    "\n",
    "1. Validate the user-supplied data\n",
    "2. Create a data loader\n",
    "3. Iterate over the data loader and feed into the VAE, getting the tensor of interest out of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PehIGudfDXRr"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_latent_representation(\n",
    "    self,\n",
    "    adata: Optional[AnnData] = None,\n",
    "    indices: Optional[Sequence[int]] = None,\n",
    "    batch_size: Optional[int] = None,\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Return the latent representation for each cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "        AnnData object used to initialize the model.\n",
    "    indices\n",
    "        Indices of cells in adata to use. If `None`, all cells are used.\n",
    "    batch_size\n",
    "        Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    latent_representation : np.ndarray\n",
    "        Low-dimensional representation for each cell\n",
    "    \"\"\"\n",
    "    if self.is_trained_ is False:\n",
    "        raise RuntimeError(\"Please train the model first.\")\n",
    "\n",
    "    adata = self._validate_anndata(adata)\n",
    "    dataloader = self._make_dataloader(adata=adata, indices=indices, batch_size=batch_size)\n",
    "    latent = []\n",
    "    for tensors in dataloader:\n",
    "        inference_inputs = self.module._get_inference_input(tensors)\n",
    "        outputs = self.module.inference(**inference_inputs)\n",
    "        qz_m = outputs[\"qz_m\"]\n",
    "\n",
    "        latent += [qz_m.cpu()]\n",
    "    return torch.cat(latent).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hhvcl6zGGIV"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "Validating the anndata is critical to the user experience. If `None` is passed it just returns the anndata used to initialize the model, but if a different object is passed, it checks that this new object is equivalent in structure to the anndata passed to the model. We took great care in engineering this function so as to allow passing anndata objects with potentially missing categories (e.g., model was trained on batches `[\"A\", \"B\", \"C\"]`, but the passed anndata only has `[\"B\", \"C\"]`). These sorts of checks will ensure that your module will see data that it expects, and the user will get the results they expect without advanced data manipulations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4a-4Ssmu6qk"
   },
   "source": [
    "As a convention, we like to keep the module code as bare as possible and leave all posterior manipulation of module tensors to the model class methods. However, it would have been possible to write a `get_z` method in the module, and just have the model class that method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmsozWgrDObo"
   },
   "source": [
    "## Mixing in pre-coded features\n",
    "\n",
    "We have a number of Mixin classes that can add functionality to your model through inheritance. Here we demonstrate the [`VAEMixin`](https://www.scvi-tools.org/en/stable/api/reference/scvi.model.base.VAEMixin.html#scvi.model.base.VAEMixin) class.\n",
    "\n",
    "Let's try to get the latent representation from the object we already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KntvOQekxq0K",
    "outputId": "93cd2a5a-7ac7-4437-926d-b43caff8cdc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This function does not exist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.get_latent_representation()\n",
    "except AttributeError:\n",
    "    print(\"This function does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPf4xHI_L-oI"
   },
   "source": [
    "This method becomes avaialble once the `VAEMixin` is inherited. Here's an overview of the mixin methods, which are coded generally enough that they should be broadly useful to those building VAEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QD3HWo2lLKSC"
   },
   "source": [
    "```python\n",
    "class VAEMixin:\n",
    "    @torch.no_grad()\n",
    "    def get_elbo(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "    ) -> float:\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_marginal_ll(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        n_mc_samples: int = 1000,\n",
    "        batch_size: Optional[int] = None,\n",
    "    ) -> float:\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_reconstruction_error(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "    ) -> Union[float, Dict[str, float]]:\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_latent_representation(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        give_mean: bool = True,\n",
    "        mc_samples: int = 5000,\n",
    "        batch_size: Optional[int] = None,\n",
    "    ) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA2ES0nANQN8"
   },
   "source": [
    "Let's now inherit the mixin into our SCVI class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Lu4Rb9nw2217"
   },
   "outputs": [],
   "source": [
    "from scvi.model.base import VAEMixin, UnsupervisedTrainingMixin\n",
    "\n",
    "class SCVI(VAEMixin, UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"\n",
    "    single-cell Variational Inference [Lopez18]_.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_latent: int = 10,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super(SCVI, self).__init__(adata)\n",
    "\n",
    "        self.module = VAE(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            n_batch=self.summary_stats[\"n_batch\"],\n",
    "            n_latent=n_latent,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            \"SCVI Model with the following params: \\nn_latent: {}\"\n",
    "        ).format(\n",
    "            n_latent,\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8erM-DwwNV9V",
    "outputId": "f15996d8-e22d-4dc9-eb84-4c735b9d34ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10/10 [00:00<00:00, 43.13it/s, loss=312, v_num=1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7.24648237e-01, -6.79293945e-02,  2.24610746e-01, ...,\n",
       "        -1.60631642e-01, -4.38390583e-01, -1.13472319e+00],\n",
       "       [-8.44153583e-01, -4.75247622e-01,  2.80124843e-02, ...,\n",
       "        -1.94080174e-04,  8.48569334e-01,  3.32585931e-01],\n",
       "       [-1.54564455e-01, -2.80499250e-01, -1.11564890e-01, ...,\n",
       "        -9.88644898e-01,  6.64949536e-01, -7.96886533e-02],\n",
       "       ...,\n",
       "       [ 4.71180618e-01, -5.71987391e-01, -4.22892049e-02, ...,\n",
       "        -3.70038971e-02,  2.39081487e-01, -2.67369717e-01],\n",
       "       [ 6.74252212e-01, -7.91834950e-01, -1.84910953e-01, ...,\n",
       "        -6.05610073e-01,  1.00632414e-01,  3.61604303e-01],\n",
       "       [ 1.94981873e-01, -7.31289536e-02, -8.98141861e-02, ...,\n",
       "        -3.45393956e-01, -4.50782865e-01, -3.28205645e-01]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SCVI(adata)\n",
    "model.train(10)\n",
    "model.get_latent_representation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWvfa7q9NggC"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59_79HYQNjv_"
   },
   "source": [
    "We learned the structure of the high-level model classes in scvi-tools, and learned how a simple version of `SCVI` is implemented.\n",
    "\n",
    "Questions? Comments? Keep the discussion going on our [forum](https://discourse.scvi-tools.org/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model_user_guide.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
